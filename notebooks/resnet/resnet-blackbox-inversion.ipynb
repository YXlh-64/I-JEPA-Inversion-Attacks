{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d047874d",
   "metadata": {},
   "source": [
    "## ResNet-50 Black-Box Feature Inversion Attack\n",
    "\n",
    "This script implements the black-box feature inversion attack on ResNet-50 as described in \n",
    "the research paper \"Inverting Features with Diffusion Priors\".\n",
    "\n",
    "The implementation includes three baseline models for comparison:\n",
    "1. DO (Direct Output): Direct reconstruction without LDM\n",
    "2. DB (Decoder-Based): Integrated LDM decoder  \n",
    "3. DMB (Diffusion-based Model with Black-box): U-Net + Frozen LDM Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9cf5904-bc44-4c79-bbee-ab804572cf83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:26:10.609590Z",
     "iopub.status.busy": "2025-08-24T08:26:10.609227Z",
     "iopub.status.idle": "2025-08-24T08:26:14.313805Z",
     "shell.execute_reply": "2025-08-24T08:26:14.307702Z",
     "shell.execute_reply.started": "2025-08-24T08:26:10.609564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lpips in /usr/local/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/site-packages (from lpips) (2.6.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from lpips) (1.15.3)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/site-packages (from lpips) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/site-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/site-packages (from lpips) (2.0.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.18.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (11.2.1.3)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (4.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (11.6.1.9)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (10.3.5.147)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision>=0.2.1->lpips) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c95ff06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:34:45.892031Z",
     "iopub.status.busy": "2025-08-24T08:34:45.891700Z",
     "iopub.status.idle": "2025-08-24T08:34:45.907997Z",
     "shell.execute_reply": "2025-08-24T08:34:45.904079Z",
     "shell.execute_reply.started": "2025-08-24T08:34:45.892004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Configuration: {'seed': 42, 'device': 'cpu', 'batch_size': 128, 'epochs': 96, 'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'lambda_s': 1.0, 'training_samples': 4096, 'testing_samples': 1024, 'image_size': 224, 'latent_size': 64, 'latent_channels': 4, 'results_dir': 'results_resnet50_blackbox', 'checkpoint_dir': 'checkpoints_resnet50_blackbox'}\n"
     ]
    }
   ],
   "source": [
    "# imports and config \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from diffusers import AutoencoderKL\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil \n",
    "import random\n",
    "from typing import Optional, Union, List, Tuple, Dict, Any\n",
    "import lpips\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration based on paper's experimental settings\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'batch_size': 128,  # Paper uses batch size 128\n",
    "    'epochs': 96,       # Paper uses 96 epochs\n",
    "    'lr': 0.1,          # Paper uses initial learning rate 0.1\n",
    "    'beta1': 0.9,       # Paper uses Adam optimizer with beta=(0.9, 0.999)\n",
    "    'beta2': 0.999,\n",
    "    'lambda_s': 1.0,    # Paper uses λ_s = 1 for equations 11, 12, 13\n",
    "    'training_samples': 4096,  # Paper uses 4096 training images\n",
    "    'testing_samples': 1024,   # Paper uses 1024 testing images\n",
    "    'image_size': 224,  # ResNet-50 standard input size\n",
    "    'latent_size': 64,  # LDM latent size (512/8 = 64 for 8x downsampling)\n",
    "    'latent_channels': 4,  # LDM latent channels\n",
    "    'results_dir': 'results_resnet50_blackbox',\n",
    "    'checkpoint_dir': 'checkpoints_resnet50_blackbox'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Configuration: {CONFIG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069a0c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:07:57.972624Z",
     "iopub.status.busy": "2025-08-24T08:07:57.972312Z",
     "iopub.status.idle": "2025-08-24T08:07:57.985236Z",
     "shell.execute_reply": "2025-08-24T08:07:57.979418Z",
     "shell.execute_reply.started": "2025-08-24T08:07:57.972598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet50Wrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for ResNet-50 model to extract features\n",
    "    Based on the paper's target model F₁(.)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet-50\n",
    "        self.model = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the final classification layer to get features\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        # ResNet-50 features: [B, 2048, 1, 1] -> [B, 2048]\n",
    "        self.feature_dim = 2048\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from the last layer before classification\n",
    "        features = self.model(x)\n",
    "        # Flatten features: [B, 2048, 1, 1] -> [B, 2048]\n",
    "        features = features.view(features.size(0), -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd80063e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:07:59.959386Z",
     "iopub.status.busy": "2025-08-24T08:07:59.959094Z",
     "iopub.status.idle": "2025-08-24T08:07:59.975895Z",
     "shell.execute_reply": "2025-08-24T08:07:59.971517Z",
     "shell.execute_reply.started": "2025-08-24T08:07:59.959360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UNetInversion(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net component of the inversion DNN F_u(.)\n",
    "    Takes ResNet-50 features as input and generates latent variables for LDM\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, latent_channels=4, latent_size=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_channels = latent_channels\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # U-Net architecture for feature to latent mapping\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Linear(2048, latent_channels * latent_size * latent_size),\n",
    "            nn.Tanh()  # Output in [-1, 1] range for LDM\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(e3)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        d3 = self.dec3(bottleneck + e3)\n",
    "        d2 = self.dec2(d3 + e2)\n",
    "        d1 = self.dec1(d2 + e1)\n",
    "        \n",
    "        # Reshape to latent format [B, C, H, W]\n",
    "        latent = d1.view(-1, self.latent_channels, self.latent_size, self.latent_size)\n",
    "        \n",
    "        return latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757dd42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:02.117300Z",
     "iopub.status.busy": "2025-08-24T08:08:02.116995Z",
     "iopub.status.idle": "2025-08-24T08:08:02.129995Z",
     "shell.execute_reply": "2025-08-24T08:08:02.124891Z",
     "shell.execute_reply.started": "2025-08-24T08:08:02.117275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Updated InversionDNN class - Replace the existing cell\n",
    "class InversionDNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete inversion DNN F_θ^inv(.) as described in the paper\n",
    "    Consists of U-Net F_u(.) and LDM decoder D(.)\n",
    "    FIXED: Added resizing to match input image dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, latent_channels=4, latent_size=64, target_size=224):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.unet = UNetInversion(input_dim, latent_channels, latent_size)\n",
    "        \n",
    "        # Load pre-trained LDM components\n",
    "        # Using Stable Diffusion's VAE decoder\n",
    "        self.ldm_decoder = AutoencoderKL.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            subfolder=\"vae\"\n",
    "        ).decoder\n",
    "        \n",
    "        # Freeze LDM decoder parameters\n",
    "        for param in self.ldm_decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.ldm_decoder.eval()\n",
    "        \n",
    "    def forward(self, features):\n",
    "        # U-Net generates latent variables\n",
    "        latent = self.unet(features)\n",
    "        \n",
    "        # Scale latent to match LDM's expected range\n",
    "        latent = latent * 0.18215  # LDM scaling factor\n",
    "        \n",
    "        # LDM decoder reconstructs the image (outputs 512x512)\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.ldm_decoder(latent)\n",
    "        \n",
    "        # Resize to match target image size using interpolation\n",
    "        if reconstructed.shape[-1] != self.target_size:\n",
    "            reconstructed = F.interpolate(\n",
    "                reconstructed, \n",
    "                size=(self.target_size, self.target_size), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7a988",
   "metadata": {},
   "source": [
    "### Direct Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b58f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:04.454115Z",
     "iopub.status.busy": "2025-08-24T08:08:04.453775Z",
     "iopub.status.idle": "2025-08-24T08:08:04.468109Z",
     "shell.execute_reply": "2025-08-24T08:08:04.462445Z",
     "shell.execute_reply.started": "2025-08-24T08:08:04.454088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DOInversionDNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DO (Direct Output) variant\n",
    "    Directly reconstructs user input x without relying on LDM\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, output_channels=3, output_size=224):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Direct reconstruction network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.LayerNorm(4096),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(4096, 8192),\n",
    "            nn.LayerNorm(8192),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(8192, 16384),\n",
    "            nn.LayerNorm(16384),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(16384, output_channels * output_size * output_size),\n",
    "            nn.Tanh()  # Output in [-1, 1] range\n",
    "        )\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.network(features)\n",
    "        return output.view(-1, self.output_channels, self.output_size, self.output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad856e",
   "metadata": {},
   "source": [
    "### Decoder Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9df341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:08.900016Z",
     "iopub.status.busy": "2025-08-24T08:08:08.899725Z",
     "iopub.status.idle": "2025-08-24T08:08:08.911078Z",
     "shell.execute_reply": "2025-08-24T08:08:08.906993Z",
     "shell.execute_reply.started": "2025-08-24T08:08:08.899991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Updated DBInversionDNN class - Replace the existing cell\n",
    "class DBInversionDNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DB (Decoder-Based) variant\n",
    "    Integrates LDM decoder into the inversion DNN\n",
    "    FIXED: Added resizing to match input image dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, latent_channels=4, latent_size=64, target_size=224):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.unet = UNetInversion(input_dim, latent_channels, latent_size)\n",
    "        \n",
    "        # Integrated LDM decoder (trainable)\n",
    "        self.ldm_decoder = AutoencoderKL.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            subfolder=\"vae\"\n",
    "        ).decoder\n",
    "        \n",
    "        # Make LDM decoder trainable for DB variant\n",
    "        for param in self.ldm_decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    def forward(self, features):\n",
    "        latent = self.unet(features)\n",
    "        latent = latent * 0.18215\n",
    "        reconstructed = self.ldm_decoder(latent)\n",
    "        \n",
    "        # Resize to match target image size using interpolation\n",
    "        if reconstructed.shape[-1] != self.target_size:\n",
    "            reconstructed = F.interpolate(\n",
    "                reconstructed, \n",
    "                size=(self.target_size, self.target_size), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca5646",
   "metadata": {},
   "source": [
    "### Defining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a914ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:12.305643Z",
     "iopub.status.busy": "2025-08-24T08:08:12.305363Z",
     "iopub.status.idle": "2025-08-24T08:08:12.318315Z",
     "shell.execute_reply": "2025-08-24T08:08:12.314105Z",
     "shell.execute_reply.started": "2025-08-24T08:08:12.305619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BlackBoxDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for black-box feature inversion\n",
    "    Creates pairs of (input_image, resnet_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, resnet_model, transform=None, num_samples=None):\n",
    "        self.dataset = dataset\n",
    "        self.resnet_model = resnet_model\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples if num_samples else len(dataset)\n",
    "        \n",
    "        # Limit dataset size\n",
    "        self.indices = list(range(min(self.num_samples, len(dataset))))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        \n",
    "        if isinstance(self.dataset, datasets.ImageFolder):\n",
    "            image, _ = self.dataset[actual_idx]\n",
    "        else:\n",
    "            image = self.dataset[actual_idx]\n",
    "            \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Extract ResNet-50 features\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet_model(image.unsqueeze(0))\n",
    "            features = features.squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "        return image, features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4452f29",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2239ed1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:15.557422Z",
     "iopub.status.busy": "2025-08-24T08:08:15.556976Z",
     "iopub.status.idle": "2025-08-24T08:08:15.570511Z",
     "shell.execute_reply": "2025-08-24T08:08:15.564959Z",
     "shell.execute_reply.started": "2025-08-24T08:08:15.557384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    \"\"\"\n",
    "    Total Variation loss for smoothness\n",
    "    TV = Σ |x[i,j] - x[i,j-1]| + |x[i,j] - x[i-1,j]|\n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    h_x = x.size(2)\n",
    "    w_x = x.size(3)\n",
    "    \n",
    "    count_h = h_x * w_x\n",
    "    count_w = h_x * w_x\n",
    "    \n",
    "    h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x-1, :]), 2).sum()\n",
    "    w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x-1]), 2).sum()\n",
    "    \n",
    "    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "def reconstruction_loss(pred, target):\n",
    "    \"\"\"L1 reconstruction loss\"\"\"\n",
    "    return F.l1_loss(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ea86e",
   "metadata": {},
   "source": [
    "### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6f97e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:17.870299Z",
     "iopub.status.busy": "2025-08-24T08:08:17.869946Z",
     "iopub.status.idle": "2025-08-24T08:08:17.892662Z",
     "shell.execute_reply": "2025-08-24T08:08:17.886462Z",
     "shell.execute_reply.started": "2025-08-24T08:08:17.870269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_inversion_dnn(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Train the inversion DNN according to paper's specifications\n",
    "    \"\"\"\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer: Adam with paper's hyperparameters\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(config['beta1'], config['beta2'])\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    recon_criterion = reconstruction_loss\n",
    "    tv_criterion = total_variation_loss\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, features) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]}')):\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(features)\n",
    "            \n",
    "            # Compute loss according to equation 11\n",
    "            recon_loss = recon_criterion(reconstructed, images)\n",
    "            tv_loss = tv_criterion(reconstructed)\n",
    "            \n",
    "            total_loss = recon_loss + config['lambda_s'] * tv_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += total_loss.item()\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, features in val_loader:\n",
    "                images = images.to(device)\n",
    "                features = features.to(device)\n",
    "                \n",
    "                reconstructed = model(features)\n",
    "                \n",
    "                recon_loss = recon_criterion(reconstructed, images)\n",
    "                tv_loss = tv_criterion(reconstructed)\n",
    "                \n",
    "                total_loss = recon_loss + config['lambda_s'] * tv_loss\n",
    "                val_loss += total_loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f\"{config['checkpoint_dir']}/best_model.pth\")\n",
    "            \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f\"{config['checkpoint_dir']}/checkpoint_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['results_dir']}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa17ada",
   "metadata": {},
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4524bc41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:08:21.768891Z",
     "iopub.status.busy": "2025-08-24T08:08:21.768597Z",
     "iopub.status.idle": "2025-08-24T08:08:21.788975Z",
     "shell.execute_reply": "2025-08-24T08:08:21.783429Z",
     "shell.execute_reply.started": "2025-08-24T08:08:21.768865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, config):\n",
    "    \"\"\"\n",
    "    Evaluate the trained inversion DNN\n",
    "    \"\"\"\n",
    "    device = config['device']\n",
    "    model.eval()\n",
    "    \n",
    "    # Metrics\n",
    "    recon_losses = []\n",
    "    tv_losses = []\n",
    "    lpips_scores = []\n",
    "    \n",
    "    # LPIPS for perceptual similarity\n",
    "    lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, features in tqdm(test_loader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "            \n",
    "            reconstructed = model(features)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            recon_loss = reconstruction_loss(reconstructed, images)\n",
    "            recon_losses.append(recon_loss.item())\n",
    "            \n",
    "            # Total variation loss\n",
    "            tv_loss = total_variation_loss(reconstructed)\n",
    "            tv_losses.append(tv_loss.item())\n",
    "            \n",
    "            # LPIPS score\n",
    "            lpips_score = lpips_fn(reconstructed, images).mean()\n",
    "            lpips_scores.append(lpips_score.item())\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_recon_loss = np.mean(recon_losses)\n",
    "    avg_tv_loss = np.mean(tv_losses)\n",
    "    avg_lpips = np.mean(lpips_scores)\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Average Reconstruction Loss: {avg_recon_loss:.4f}\")\n",
    "    print(f\"Average Total Variation Loss: {avg_tv_loss:.4f}\")\n",
    "    print(f\"Average LPIPS Score: {avg_lpips:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'recon_loss': avg_recon_loss,\n",
    "        'tv_loss': avg_tv_loss,\n",
    "        'lpips': avg_lpips\n",
    "    }\n",
    "\n",
    "def visualize_results(model, test_loader, config, num_samples=8):\n",
    "    \"\"\"\n",
    "    Visualize reconstruction results\n",
    "    \"\"\"\n",
    "    device = config['device']\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of samples\n",
    "    images, features = next(iter(test_loader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    features = features[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(features)\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    images_np = images.cpu().numpy()\n",
    "    reconstructed_np = reconstructed.cpu().numpy()\n",
    "    \n",
    "    # Denormalize images (assuming ImageNet normalization)\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "    \n",
    "    images_np = images_np * std + mean\n",
    "    reconstructed_np = reconstructed_np * std + mean\n",
    "    \n",
    "    # Clip to [0, 1]\n",
    "    images_np = np.clip(images_np, 0, 1)\n",
    "    reconstructed_np = np.clip(reconstructed_np, 0, 1)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(2*num_samples, 4))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(np.transpose(images_np[i], (1, 2, 0)))\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstructed image\n",
    "        axes[1, i].imshow(np.transpose(reconstructed_np[i], (1, 2, 0)))\n",
    "        axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['results_dir']}/reconstruction_results.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7e7ee83-a50d-4efb-a188-26c356408f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:26:32.969233Z",
     "iopub.status.busy": "2025-08-24T08:26:32.968870Z",
     "iopub.status.idle": "2025-08-24T08:28:29.067477Z",
     "shell.execute_reply": "2025-08-24T08:28:29.062696Z",
     "shell.execute_reply.started": "2025-08-24T08:26:32.969204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-24 08:26:33--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
      "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
      "--2025-08-24 08:26:33--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248100043 (237M) [application/zip]\n",
      "Saving to: ‘tiny-imagenet-200.zip’\n",
      "\n",
      "tiny-imagenet-200.z 100%[===================>] 236.61M  3.53MB/s    in 1m 45s  \n",
      "\n",
      "2025-08-24 08:28:18 (2.26 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip -q tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4081220b-5821-495c-872f-241ba1dfd36f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T08:39:17.551882Z",
     "iopub.status.busy": "2025-08-24T08:39:17.551539Z",
     "iopub.status.idle": "2025-08-24T08:39:17.568293Z",
     "shell.execute_reply": "2025-08-24T08:39:17.561725Z",
     "shell.execute_reply.started": "2025-08-24T08:39:17.551851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_tinyimagenet_subset(src_dir=\"tiny-imagenet-200/train\",\n",
    "                               subset_dir=\"tiny_subset\",\n",
    "                               num_train=4096,\n",
    "                               num_test=1024,\n",
    "                               seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Collect all (image_path, class_id)\n",
    "    class_folders = [d for d in os.listdir(src_dir)\n",
    "                     if os.path.isdir(os.path.join(src_dir, d))]\n",
    "    all_images = []\n",
    "    for cls in class_folders:\n",
    "        img_dir = os.path.join(src_dir, cls, \"images\")\n",
    "        for img in os.listdir(img_dir):\n",
    "            if img.endswith(\".JPEG\"):\n",
    "                all_images.append((os.path.join(img_dir, img), cls))\n",
    "\n",
    "    if len(all_images) < (num_train + num_test):\n",
    "        raise RuntimeError(f\"Not enough images in Tiny-ImageNet: found {len(all_images)}\")\n",
    "\n",
    "    random.shuffle(all_images)\n",
    "    train_imgs = all_images[:num_train]\n",
    "    test_imgs  = all_images[num_train:num_train+num_test]\n",
    "\n",
    "    # Create split dirs\n",
    "    for split in (\"train\", \"test\"):\n",
    "        os.makedirs(os.path.join(subset_dir, split), exist_ok=True)\n",
    "\n",
    "    # Copy while preserving class folders\n",
    "    for path, cls in train_imgs:\n",
    "        dst = os.path.join(subset_dir, \"train\", cls)\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "        shutil.copy(path, dst)\n",
    "\n",
    "    for path, cls in test_imgs:\n",
    "        dst = os.path.join(subset_dir, \"test\", cls)\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "        shutil.copy(path, dst)\n",
    "\n",
    "    print(f\"[subset] {num_train} train, {num_test} test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a43b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:01:36.802029Z",
     "iopub.status.busy": "2025-08-24T09:01:36.801690Z",
     "iopub.status.idle": "2025-08-24T09:01:36.822582Z",
     "shell.execute_reply": "2025-08-24T09:01:36.818327Z",
     "shell.execute_reply.started": "2025-08-24T09:01:36.802000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing ResNet-50 Black-Box Inversion Attack...\")\n",
    "\n",
    "    # Target model (ResNet-50)\n",
    "    print(\"Loading ResNet-50 model...\")\n",
    "    resnet_model = ResNet50Wrapper(pretrained=True)\n",
    "    resnet_model.eval()\n",
    "\n",
    "    # Transforms\n",
    "    transform = T.Compose([\n",
    "        T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Ensure Tiny-ImageNet is present\n",
    "    if not os.path.exists(\"tiny-imagenet-200\"):\n",
    "        os.system(\"wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\")\n",
    "        os.system(\"unzip -q tiny-imagenet-200.zip\")\n",
    "\n",
    "    # Build subset with preserved class structure\n",
    "    create_tinyimagenet_subset(\n",
    "        src_dir=\"tiny-imagenet-200/train\",\n",
    "        subset_dir=\"tiny_subset\",\n",
    "        num_train=CONFIG['training_samples'],\n",
    "        num_test=CONFIG['testing_samples']\n",
    "    )\n",
    "\n",
    "    # ImageFolder datasets\n",
    "    train_data = datasets.ImageFolder(root=\"tiny_subset/train\", transform=transform)\n",
    "    test_data  = datasets.ImageFolder(root=\"tiny_subset/test\",  transform=transform)\n",
    "\n",
    "    # Wrap with your black-box dataset (no extra transform needed here)\n",
    "    train_dataset = BlackBoxDataset(train_data, resnet_model)\n",
    "    test_dataset  = BlackBoxDataset(test_data,  resnet_model)\n",
    "\n",
    "    # Split train into train/val (90/10)\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size   = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # DataLoaders\n",
    "    num_workers = min(4, os.cpu_count() or 1)\n",
    "    pin_mem = torch.cuda.is_available()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_mem)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin_mem)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin_mem)\n",
    "\n",
    "    print(f\"Training samples:  {len(train_dataset)}\")\n",
    "    print(f\"Validation samples:{len(val_dataset)}\")\n",
    "    print(f\"Testing samples:   {len(test_dataset)}\")\n",
    "\n",
    "    # UPDATED: Inversion models with target_size parameter\n",
    "    print(\"Initializing inversion DNN models...\")\n",
    "    models = {\n",
    "        'DMB': InversionDNN(2048, target_size=CONFIG['image_size']),  # FIXED: Added target_size\n",
    "        'DO' : DOInversionDNN(2048),\n",
    "        'DB' : DBInversionDNN(2048, target_size=CONFIG['image_size']),  # FIXED: Added target_size\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Training {name} model...\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        trained_model, train_losses, val_losses = train_inversion_dnn(\n",
    "            model, train_loader, val_loader, CONFIG\n",
    "        )\n",
    "        metrics = evaluate_model(trained_model, test_loader, CONFIG)\n",
    "        visualize_results(trained_model, test_loader, CONFIG)\n",
    "\n",
    "        results[name] = {\n",
    "            'model': trained_model,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "\n",
    "        # NOTE: use CONFIG (not config)\n",
    "        os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "        torch.save(trained_model.state_dict(),\n",
    "                   f\"{CONFIG['checkpoint_dir']}/{name}_final.pth\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULTS COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Reconstruction Loss: {result['metrics']['recon_loss']:.4f}\")\n",
    "        print(f\"  Total Variation Loss: {result['metrics']['tv_loss']:.4f}\")\n",
    "        print(f\"  LPIPS Score:         {result['metrics']['lpips']:.4f}\")\n",
    "\n",
    "    print(f\"\\nResults saved to:     {CONFIG['results_dir']}\")\n",
    "    print(f\"Checkpoints saved to: {CONFIG['checkpoint_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69c6b5ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:01:37.918969Z",
     "iopub.status.busy": "2025-08-24T09:01:37.918686Z",
     "iopub.status.idle": "2025-08-24T09:10:13.160738Z",
     "shell.execute_reply": "2025-08-24T09:10:13.154206Z",
     "shell.execute_reply.started": "2025-08-24T09:01:37.918937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResNet-50 Black-Box Inversion Attack...\n",
      "Loading ResNet-50 model...\n",
      "Subset created: 4096 train, 1024 test images\n",
      "Training samples:  3686\n",
      "Validation samples:410\n",
      "Testing samples:   1024\n",
      "Initializing inversion DNN models...\n",
      "\n",
      "==================================================\n",
      "Training DMB model...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/96:   0%|          | 0/29 [00:00<?, ?it/s]/tmp/ipykernel_10/31578568.py:20: UserWarning: Using a target size (torch.Size([128, 3, 224, 224])) that is different to the input size (torch.Size([128, 3, 512, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(pred, target)\n",
      "Epoch 1/96:   0%|          | 0/29 [06:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (224) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m trained_model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_inversion_dnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, test_loader, CONFIG)\n\u001b[1;32m     81\u001b[0m visualize_results(trained_model, test_loader, CONFIG)\n",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mtrain_inversion_dnn\u001b[0;34m(model, train_loader, val_loader, config)\u001b[0m\n\u001b[1;32m     34\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compute loss according to equation 11\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrecon_criterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m tv_loss \u001b[38;5;241m=\u001b[39m tv_criterion(reconstructed)\n\u001b[1;32m     40\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_s\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m tv_loss\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mreconstruction_loss\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreconstruction_loss\u001b[39m(pred, target):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"L1 reconstruction loss\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:3810\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3808\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3810\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (224) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8175fd-9d63-4cee-8b93-438ec0c81476",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
